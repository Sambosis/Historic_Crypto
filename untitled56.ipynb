{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObMlaEV/eXp9wQd1zY4e9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sambosis/Historic_Crypto/blob/main/untitled56.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "U3Qf9ojRZT0O",
        "outputId": "c9b585a2-bc2c-402b-b673-4aeb006893cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU cores available: 8\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msambosis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20240930_224618-2fvh1gw7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sambosis/my-awesome-project/runs/2fvh1gw7' target=\"_blank\">sparkling-microwave-462</a></strong> to <a href='https://wandb.ai/sambosis/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sambosis/my-awesome-project' target=\"_blank\">https://wandb.ai/sambosis/my-awesome-project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sambosis/my-awesome-project/runs/2fvh1gw7' target=\"_blank\">https://wandb.ai/sambosis/my-awesome-project/runs/2fvh1gw7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-645818ec96e4>\u001b[0m in \u001b[0;36m<cell line: 848>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0;31m# Start Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Skipping training as TRAIN_FIRST is set to False.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_method\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"fork\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forkserver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0m_check_bad_cuda_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0m_check_missing_main_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_IS_INTERACTIVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" You will have to restart the Python kernel.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "# Uncomment the following lines if running in a new environment\n",
        "# !pip install fluidstack -q\n",
        "# !pip install pytorch_lightning tensorflow icecream tensorboardX rich wandb -q\n",
        "\n",
        "# Standard Library Imports\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "import multiprocessing\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "# Third-Party Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, RichProgressBar\n",
        "from icecream import ic\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.text import Text\n",
        "from rich.box import ROUNDED\n",
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "import torch.distributed as dist\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "# Initialize Rich Console\n",
        "console = Console()\n",
        "num_cpus = multiprocessing.cpu_count()\n",
        "print(f\"Number of CPU cores available: {num_cpus}\")\n",
        "\n",
        "# Configuration Dataclass\n",
        "@dataclass\n",
        "class Config:\n",
        "    # read the file \"version\" and increment the version number\n",
        "    with open(\"version\", \"r\") as f:\n",
        "        version = int(f.read())\n",
        "        VERSION_N = version + 1\n",
        "        # print(f\"Version number: {VERSION_N}\")\n",
        "        f.close()\n",
        "    with open(\"version\", \"w\") as f:\n",
        "        f.write(str(VERSION_N))\n",
        "        f.close()\n",
        "    # VERSION_N: int = 87\n",
        "    RECORDS_TO_LOAD: int = 1205040\n",
        "    N_PAST: int = 3 * 12 * 3  # 1 week of 10-minute intervals\n",
        "    N_FUTURE: int = 1 * 12 * 2  # 1 day of 10-minute intervals\n",
        "    BATCH_SIZE: int = 2000\n",
        "    HIDDEN_SIZE: int = 256\n",
        "    NUM_LAYERS: int = 2\n",
        "    NUM_EPOCHS: int = 150\n",
        "    HOT_RESTART: bool = False\n",
        "    TRAIN_FIRST: bool = True\n",
        "    EPOCH_TO_RESTART: int = 50\n",
        "    BATCH_FACTOR: int = 81\n",
        "    DEBUG_FREQ: int = 180\n",
        "    num_cpus = multiprocessing.cpu_count()\n",
        "    NUM_WORKERS = (num_cpus // 4 - 4) if num_cpus > 16 else 4\n",
        "    DEBUG_ON: bool = False\n",
        "    DATA_URL: str = 'https://sambo.us-iad-1.linodeobjects.com/fillnan_combined_df.csv'\n",
        "    DATA_FILE: str = './data/fill_nan_df.csv'\n",
        "    MODEL_PATH: str = \"/teamspace/studios/this_studio/models/TransformerModel351/model-351-epoch=148-val_loss=0.71.ckpt\"\n",
        "    MODEL_SAVE_PATH: str = f'./models/TransformerModel{VERSION_N}'\n",
        "    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    EPSILON: float = 1e-4\n",
        "\n",
        "# Initialize Configuration\n",
        "cfg = Config()\n",
        "\n",
        "# Set Random Seed for Reproducibility\n",
        "pl.seed_everything(40, workers=True)\n",
        "\n",
        "# Print Device Information\n",
        "print(f\"Using device: {cfg.DEVICE}\")\n",
        "os.makedirs(cfg.MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Initialize IceCream Debugging\n",
        "if cfg.DEBUG_ON:\n",
        "    ic.enable()\n",
        "else:\n",
        "    ic.disable()\n",
        "\n",
        "# Callback to Update Percentile Cutoff\n",
        "class UpdatePercentileCutoffCallback(Callback):\n",
        "    def __init__(self, reduction_threshold=1.9, reduction_factor=0.9):\n",
        "        super().__init__()\n",
        "        self.reduction_threshold = reduction_threshold\n",
        "        self.reduction_factor = reduction_factor\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        # Skip during sanity check to prevent freezing\n",
        "        if trainer.sanity_checking:\n",
        "            return\n",
        "\n",
        "        # Only the main process (rank 0) determines if reduction is needed\n",
        "        if trainer.is_global_zero:\n",
        "            avg_reward = trainer.callback_metrics.get('val/reward', 0)\n",
        "\n",
        "            if avg_reward > self.reduction_threshold:\n",
        "                old_perc_cutoff = pl_module.criterion.get_perc_cutoff()\n",
        "                new_perc_cutoff = old_perc_cutoff * self.reduction_factor\n",
        "                pl_module.criterion.set_perc_cutoff(new_perc_cutoff)\n",
        "                pl_module.criterion.perc_cutoff_buffer.fill_(new_perc_cutoff)\n",
        "                print(f\"PercentileCutoffCallback: Reducing perc_cutoff from {old_perc_cutoff:.5f} to {new_perc_cutoff:.5f}\")\n",
        "\n",
        "                # Log reduction event to WandB\n",
        "                pl_module.logger.experiment.log({\n",
        "                    \"percentile_cutoff_reduction\": new_perc_cutoff,\n",
        "                    \"avg_reward\": avg_reward\n",
        "                })\n",
        "\n",
        "# PositionalEncoding Class\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))  # (d_model/2,)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "# Transformer-Based Model\n",
        "class CryptoTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        d_model=256,\n",
        "        nhead=8,\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.3,\n",
        "        activation=\"gelu\",\n",
        "        n_future=24,\n",
        "        num_outputs=24,\n",
        "        max_seq_length=5000\n",
        "    ):\n",
        "        super(CryptoTransformer, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.d_model = d_model\n",
        "        self.n_future = n_future\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        # Input linear layer\n",
        "        self.input_fc = nn.Linear(input_size, d_model)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_seq_length)\n",
        "        self.pos_decoder = PositionalEncoding(d_model, max_len=max_seq_length)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "            batch_first=True  # Added for compatibility with batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "            batch_first=True  # Added for compatibility with batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Output linear layer\n",
        "        self.output_fc = nn.Linear(d_model, num_outputs)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask  # (sz, sz)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (batch_size, n_past, num_features)\n",
        "            tgt: (batch_size, n_future, num_features)\n",
        "        Returns:\n",
        "            out: (batch_size, n_future, num_outputs)\n",
        "        \"\"\"\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        # Input embedding\n",
        "        src = self.input_fc(src) * np.sqrt(self.d_model)  # (batch_size, n_past, d_model)\n",
        "        tgt = self.input_fc(tgt) * np.sqrt(self.d_model)  # (batch_size, n_future, d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        src = self.pos_encoder(src)  # (batch_size, n_past, d_model)\n",
        "        tgt = self.pos_decoder(tgt)  # (batch_size, n_future, d_model)\n",
        "\n",
        "        # Create masks\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)  # (n_future, n_future)\n",
        "\n",
        "        # Transformer forward pass\n",
        "        memory = self.transformer_encoder(src)  # (batch_size, n_past, d_model)\n",
        "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)  # (batch_size, n_future, d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        out = self.output_fc(output)  # (batch_size, n_future, num_outputs)\n",
        "\n",
        "        return out  # (batch_size, n_future, num_outputs)\n",
        "\n",
        "# Custom Balanced Loss Function\n",
        "class BalancedCryptoLoss(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BalancedCryptoLoss, self).__init__()\n",
        "        # Register perc_cutoff as a buffer for automatic synchronization\n",
        "        self.register_buffer('perc_cutoff_buffer', torch.tensor(0.015))\n",
        "        self.config = config\n",
        "        self.mse_weight = 900.0\n",
        "        self.mae_weight = 25.0\n",
        "        self.max_diff_weight = 3.0\n",
        "        self.balance_weight = 3.0\n",
        "        self.direction_weight = 0.001\n",
        "        self.mean_diff_weight = 15.0\n",
        "        self.perc_diff_weight = 15.0\n",
        "        self.within_1pct_reward_weight = 5.0\n",
        "        self.reward_scaling = 5.0\n",
        "        self.epsilon = config.EPSILON\n",
        "        self.debug_freq = config.DEBUG_FREQ\n",
        "        self.epoch = 0\n",
        "        self.mean_mean_diff = 0.0\n",
        "        self.reward = 0.0\n",
        "\n",
        "    def directional_loss(self, preds, target):\n",
        "        direction_pred = (preds[:, 1:] - preds[:, :-1]).sign()\n",
        "        direction_true = (target[:, 1:] - target[:, :-1]).sign()\n",
        "\n",
        "        # Convert signs to 0 and 1\n",
        "        direction_pred = (direction_pred + 1) / 2\n",
        "        direction_true = (direction_true + 1) / 2\n",
        "\n",
        "        # Clamp values to prevent BCE from receiving exact 0 or 1\n",
        "        direction_pred = torch.clamp(direction_pred, 1e-7, 1 - 1e-7)\n",
        "        direction_true = torch.clamp(direction_true, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        return F.binary_cross_entropy(direction_pred, direction_true).mean() * self.direction_weight\n",
        "\n",
        "    def mse_loss_component(self, y_pred, y_true):\n",
        "        return F.mse_loss(y_pred, y_true) * self.mse_weight\n",
        "\n",
        "    def mae_loss_component(self, y_pred, y_true):\n",
        "        return F.l1_loss(y_pred, y_true) * self.mae_weight\n",
        "\n",
        "    def percentage_diff_component(self, y_pred, y_true):\n",
        "        perc_diff = torch.abs((y_pred - y_true) / (self.epsilon + y_true))\n",
        "        self.mean_mean_diff = torch.mean(perc_diff).item()\n",
        "        return (torch.mean(perc_diff) * self.perc_diff_weight) ** 2\n",
        "\n",
        "    def max_diff_component(self, perc_diff):\n",
        "        max_diffs, _ = torch.max(perc_diff, dim=1)\n",
        "        return torch.mean(max_diffs) * self.max_diff_weight\n",
        "\n",
        "    def imbalance_component(self, perc_diff):\n",
        "        overpredict = torch.relu(perc_diff)\n",
        "        underpredict = torch.relu(-perc_diff)\n",
        "        imbalance = torch.abs(torch.mean(overpredict, dim=1) - torch.mean(underpredict, dim=1))\n",
        "        return torch.mean(imbalance) * self.balance_weight\n",
        "\n",
        "    def reward_component(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Calculates the reward component based on the predicted and true values.\n",
        "\n",
        "        Args:\n",
        "            y_pred (torch.Tensor): The predicted values.\n",
        "            y_true (torch.Tensor): The true values.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The reward component calculated based on the percentage difference between\n",
        "                          the predicted and true values.\n",
        "        \"\"\"\n",
        "        percentage_diff = torch.abs((y_pred - y_true) / (self.epsilon + y_true))\n",
        "\n",
        "        within_1pct = (percentage_diff <= self.perc_cutoff_buffer).float()\n",
        "        within_1pct_ratio = torch.mean(within_1pct)\n",
        "        return within_1pct_ratio * self.within_1pct_reward_weight\n",
        "\n",
        "    def compute_all_losses(self, y_pred, y_true):\n",
        "        mse_loss = self.mse_loss_component(y_pred, y_true)\n",
        "        mae_loss = self.mae_loss_component(y_pred, y_true)\n",
        "        perc_diff_loss = self.percentage_diff_component(y_pred, y_true)\n",
        "        max_diff_loss = self.max_diff_component(torch.abs((y_pred - y_true) / (self.epsilon + y_true)))\n",
        "        imbalance_loss = self.imbalance_component(torch.abs((y_pred - y_true) / (self.epsilon + y_true)))\n",
        "        direction_loss = self.directional_loss(y_pred, y_true)\n",
        "        reward = self.reward_component(y_pred, y_true)\n",
        "        return mse_loss, mae_loss, perc_diff_loss, max_diff_loss, imbalance_loss, direction_loss, reward\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.epoch += 1\n",
        "\n",
        "        # Compute Loss Components\n",
        "        mse_loss, mae_loss, perc_diff_loss, max_diff_loss, imbalance_loss, direction_loss, reward = self.compute_all_losses(y_pred, y_true)\n",
        "        self.reward = reward\n",
        "        # Combine Loss Components\n",
        "        final_loss = (mse_loss + mae_loss + perc_diff_loss + max_diff_loss +\n",
        "                      imbalance_loss + direction_loss - (reward * self.reward_scaling))\n",
        "\n",
        "        # Clamp Final Loss to prevent negative values\n",
        "        final_loss = torch.clamp(final_loss, min=0.00001)\n",
        "        perc_cutoff = self.get_perc_cutoff()\n",
        "        return final_loss, mse_loss, mae_loss, perc_diff_loss, max_diff_loss, imbalance_loss, direction_loss, reward, perc_cutoff\n",
        "\n",
        "    def get_reward(self):\n",
        "        # Returns a float that is converted from a tensor\n",
        "        return self.reward.item()\n",
        "\n",
        "    def set_perc_cutoff(self, perc_cutoff):\n",
        "        # Update the buffer in-place\n",
        "        self.perc_cutoff_buffer.fill_(perc_cutoff)\n",
        "\n",
        "    def get_last_mean_diff(self):\n",
        "        return self.mean_mean_diff\n",
        "\n",
        "    def get_perc_cutoff(self):\n",
        "        return self.perc_cutoff_buffer.item()\n",
        "\n",
        "# Custom Dataset\n",
        "class CryptoDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, n_past: int, n_future: int):\n",
        "        self.data = data\n",
        "        self.n_past = n_past\n",
        "        self.n_future = n_future\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.n_past - self.n_future + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data.iloc[idx:idx + self.n_past].values  # (n_past, num_features)\n",
        "        y = self.data.iloc[idx + self.n_past:idx + self.n_past + self.n_future].values  # (n_future, num_features)\n",
        "        return torch.FloatTensor(x), torch.FloatTensor(y)\n",
        "\n",
        "# Utility Functions\n",
        "def get_random_sample(dataframe: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Retrieve a random sample from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): DataFrame to sample from.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (input_data, target_data)\n",
        "    \"\"\"\n",
        "    random_index = random.randint(0, len(dataframe) - cfg.N_PAST - cfg.N_FUTURE)\n",
        "    input_data = dataframe.iloc[random_index:random_index + cfg.N_PAST].values\n",
        "    target_data = dataframe.iloc[random_index + cfg.N_PAST:random_index + cfg.N_PAST + cfg.N_FUTURE].values\n",
        "    return torch.FloatTensor(input_data), torch.FloatTensor(target_data)\n",
        "\n",
        "def prepare_input(input_data, device):\n",
        "    \"\"\"\n",
        "    Prepare input tensor for the model.\n",
        "\n",
        "    Args:\n",
        "        input_data (torch.FloatTensor): Input data.\n",
        "\n",
        "    Returns:\n",
        "        torch.FloatTensor: Prepared input tensor.\n",
        "    \"\"\"\n",
        "    return input_data.unsqueeze(0).to(device)\n",
        "\n",
        "def convert_to_numpy(input_data, target, prediction):\n",
        "    \"\"\"\n",
        "    Convert tensors to NumPy arrays.\n",
        "\n",
        "    Args:\n",
        "        input_data (torch.FloatTensor): Input data.\n",
        "        target (torch.FloatTensor): Target data.\n",
        "        prediction (torch.FloatTensor): Prediction data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (input_np, target_np, prediction_np)\n",
        "    \"\"\"\n",
        "    return input_data.cpu().numpy(), target.cpu().numpy(), prediction.cpu().numpy()\n",
        "def gaussian_smoothing(data, window_size, sigma):\n",
        "    \"\"\"\n",
        "    Compute the Gaussian smoothing of the data.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Input data.\n",
        "        window_size (int): Window size for Gaussian smoothing.\n",
        "        sigma (float): Standard deviation of the Gaussian kernel.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Gaussian smoothed data.\n",
        "    \"\"\"\n",
        "    # Generate Gaussian kernel\n",
        "    x = np.linspace(-window_size // 2, window_size // 2, window_size)\n",
        "    kernel = np.exp(-(x ** 2) / (2 * sigma ** 2))\n",
        "    kernel /= kernel.sum()\n",
        "\n",
        "    # Convolve data with Gaussian kernel\n",
        "    return np.convolve(data, kernel, 'valid')\n",
        "\n",
        "# def gaussian_smoothing(data, window_size, sigma):\n",
        "#     \"\"\"\n",
        "#     Compute the Gaussian smoothing of the data.\n",
        "\n",
        "#     Args:\n",
        "#         data (np.ndarray): Input data.\n",
        "#         window_size (int): Window size for Gaussian smoothing.\n",
        "#         sigma (float): Standard deviation of the Gaussian kernel.\n",
        "\n",
        "#     Returns:\n",
        "#         np.ndarray: Gaussian smoothed data.\n",
        "#     \"\"\"\n",
        "#     return gaussian_filter1d(data, sigma=sigma)\n",
        "\n",
        "# Inverse Transformation Function\n",
        "def inverse_transform_predictions(scaled_value, scaler, log_transform=True):\n",
        "    \"\"\"\n",
        "    Inverse transform a scaled value back to its original scale.\n",
        "\n",
        "    Args:\n",
        "        scaled_value (np.ndarray or float): Scaled value(s).\n",
        "        scaler (MinMaxScaler): Fitted scaler used during preprocessing.\n",
        "        log_transform (bool): Indicates whether a log transform was applied.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray or float: Original scale value(s).\n",
        "    \"\"\"\n",
        "    # Ensure scaled_value is a 2D array for inverse_transform\n",
        "    scaled_array = np.array(scaled_value).reshape(-1, 1)\n",
        "    inverse_scaled = scaler.inverse_transform(scaled_array).flatten()\n",
        "\n",
        "    if log_transform:\n",
        "        original = np.exp(inverse_scaled)\n",
        "    else:\n",
        "        original = inverse_scaled\n",
        "\n",
        "    return original\n",
        "\n",
        "# Visualization Function\n",
        "def visualize_predictions(target_np, prediction_np, n_future, scalers, filtered_df, model_save_path):\n",
        "    num_features = filtered_df.shape[1]\n",
        "    max_cols = 4\n",
        "    num_rows = (num_features - 1) // max_cols + 1\n",
        "    num_cols = min(num_features, max_cols)\n",
        "\n",
        "    plt.figure(figsize=(18 * num_cols / max_cols, 6 * num_rows))\n",
        "\n",
        "    window_size = 7  # Adjust this value for smoothing\n",
        "\n",
        "    for j in range(num_features):\n",
        "        plt.subplot(num_rows, num_cols, j + 1)\n",
        "        col_name = filtered_df.columns[j]\n",
        "\n",
        "        # Extract past data from filtered_df\n",
        "        past_scaled = filtered_df[col_name].values  # Shape: (n_past,)\n",
        "        past_scaled = past_scaled[:-(n_future-1)]  # Only consider past data\n",
        "        past_inverted = inverse_transform_predictions(past_scaled, scalers[col_name])\n",
        "\n",
        "        # Directly extract the known future target data from filtered_df\n",
        "        target_scaled = filtered_df[col_name].values # Shape: (n_future,)\n",
        "        target_scaled = target_scaled[-(n_future+1):]  # Only consider future data\n",
        "        target_inverted = inverse_transform_predictions(target_scaled, scalers[col_name])\n",
        "\n",
        "        # Extract the predicted future data\n",
        "        prediction_scaled = prediction_np[0, :, j]  # Shape: (n_future,)\n",
        "        prediction_inverted = inverse_transform_predictions(prediction_scaled, scalers[col_name])\n",
        "\n",
        "        last_xbtusd_price_scaled = filtered_df['XBTUSD_price'].iloc[-1]\n",
        "        last_xbtusd_price = inverse_transform_predictions(last_xbtusd_price_scaled, scalers['XBTUSD_price'])\n",
        "\n",
        "        # Adjust if column ends with 'XBT_price'\n",
        "        if col_name.endswith('XBT_price'):\n",
        "            past_inverted *= last_xbtusd_price\n",
        "            target_inverted *= last_xbtusd_price\n",
        "            prediction_inverted *= last_xbtusd_price\n",
        "\n",
        "        # Combine past and future data\n",
        "        # total_inverted = np.concatenate((past_inverted, target_inverted))\n",
        "        total_predicted = np.concatenate((past_inverted, prediction_inverted))\n",
        "\n",
        "        # Create time indices\n",
        "        n_past = len(past_inverted)\n",
        "        total_timesteps = n_past + n_future\n",
        "        time_indices = range(total_timesteps)\n",
        "\n",
        "        # Plot past data\n",
        "        # print the lenth of the x and y axis\n",
        "        # print(len(time_indices[-(len(past_inverted)):]), len(past_inverted[n_past-n_future:]))\n",
        "\n",
        "        plt.plot(time_indices[n_past-n_future:(n_past+1)], past_inverted[-(n_future+1):], 'b', label='Past Data' if j == 0 else \"\")\n",
        "        # plt.plot(time_indices[-(len(past_inverted))+n_future:], past_inverted[:], 'b', label='Past Data' if j == 0 else \"\")\n",
        "\n",
        "        # Plot known target data\n",
        "        plt.plot(time_indices[n_past-1:], target_inverted, 'g', alpha=0.7, label='Target Data' if j == 0 else \"\")\n",
        "\n",
        "        # Plot prediction data\n",
        "        plt.plot(time_indices[n_past:], prediction_inverted, 'r', alpha=0.7, label='Prediction Data' if j == 0 else \"\")\n",
        "\n",
        "        # Optionally apply smoothing\n",
        "        total_inverted_smooth = gaussian_smoothing(past_inverted, window_size, sigma=10)\n",
        "        total_predicted_smooth = gaussian_smoothing(total_predicted, window_size, sigma=10)\n",
        "\n",
        "        # Plot smoothed data\n",
        "        # print the lenth of the x and y axis\n",
        "        # plt.plot(time_indices[n_past:], total_inverted_smooth[-n_future:], 'g', linewidth=2, label='Target Smoothed' if j == 0 else \"\")\n",
        "\n",
        "        # plt.plot(time_indices[n_past:], total_predicted_smooth[-n_future:], 'r', linewidth=2, label='Prediction Smoothed' if j == 0 else \"\")\n",
        "\n",
        "        # plt.fill_between(range(-n_future), total_predicted_smooth[:-n_future], total_inverted_smooth[:-n_future], color='blue', alpha=0.1)\n",
        "        # Ensure that both arrays have the same length for the fill_between operation\n",
        "        min_length = min(len(total_predicted_smooth), len(total_inverted_smooth))\n",
        "\n",
        "        # Adjust the indices to ensure matching lengths\n",
        "        start_index = n_past - min_length\n",
        "        end_index = n_past\n",
        "\n",
        "        # Plot smoothed data\n",
        "        # plt.plot(time_indices[n_past:], total_inverted_smooth[-n_future:], 'g', linewidth=2, label='Target Smoothed' if j == 0 else \"\")\n",
        "        plt.plot(time_indices[n_past+window_size:], total_predicted_smooth[n_past+1:], 'r', linewidth=2, label='Prediction Smoothed' if j == 0 else \"\")\n",
        "\n",
        "        # Fill between the smoothed prediction and smoothed total\n",
        "        plt.fill_between(time_indices[-n_future:], total_predicted_smooth[-n_future:], target_inverted[-n_future:], color='blue', alpha=0.1)\n",
        "        # Adjust plot settings\n",
        "        plt.title(col_name)\n",
        "        if j == 0:\n",
        "            plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    time_date = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    image_path = os.path.join(model_save_path, f\"{time_date}_predictions.png\")\n",
        "    plt.savefig(image_path)\n",
        "    plt.close()\n",
        "\n",
        "    return image_path\n",
        "\n",
        "# Checkpoint Saving Function\n",
        "def save_checkpoint(state, filename):\n",
        "    \"\"\"\n",
        "    Save a training checkpoint.\n",
        "\n",
        "    Args:\n",
        "        state (dict): State dictionary containing model, optimizer, scheduler states, etc.\n",
        "        filename (str): Path to save the checkpoint.\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n",
        "\n",
        "# Checkpoint Loading Function\n",
        "def load_checkpoint(model, optimizer, scheduler, model_path, device):\n",
        "    print(f\"Loading checkpoint from {model_path}\")\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    print(\"Checkpoint keys:\", checkpoint.keys())\n",
        "\n",
        "    # Adjust the state_dict\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['model_state_dict']\n",
        "    elif 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    else:\n",
        "        # If the checkpoint is the model's state_dict itself\n",
        "        state_dict = checkpoint\n",
        "\n",
        "    # Remove 'model.' prefix from the keys\n",
        "    from collections import OrderedDict\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('model.'):\n",
        "            name = k[6:]  # remove 'model.' prefix\n",
        "        else:\n",
        "            name = k\n",
        "        new_state_dict[name] = v\n",
        "\n",
        "    # Load the adjusted state_dict into the model\n",
        "    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "    if missing_keys:\n",
        "        print(f\"Missing keys: {missing_keys}\")\n",
        "    if unexpected_keys:\n",
        "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
        "\n",
        "    # Load optimizer and scheduler state dicts if available\n",
        "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    print(\"Checkpoint loaded successfully.\")\n",
        "    return checkpoint\n",
        "# Data Loading and Preprocessing\n",
        "def load_and_preprocess_data(file_path: str, download_url: str = None):\n",
        "    \"\"\"\n",
        "    Load and preprocess data from a CSV file. If the file does not exist, download it.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file.\n",
        "        download_url (str, optional): URL to download the CSV file. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Preprocessed DataFrame.\n",
        "        dict: Dictionary of scalers used for each column.\n",
        "    \"\"\"\n",
        "    ic(\"Starting data loading and preprocessing...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        ic(f\"File {file_path} does not exist.\")\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        if download_url:\n",
        "            ic(f\"Downloading file from {download_url}...\")\n",
        "            try:\n",
        "                response = requests.get(download_url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(file_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                ic(f\"File downloaded and saved to {file_path}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                ic(f\"Failed to download the file: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            ic(\"Download URL not provided. Cannot download the file.\")\n",
        "            raise FileNotFoundError(f\"The file {file_path} does not exist and no download URL was provided.\")\n",
        "\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "    df = df.tail(cfg.RECORDS_TO_LOAD)\n",
        "    scalers = {}\n",
        "    start_time_preprocess = time.time()\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Ensure no non-positive values before log transform\n",
        "        if (df[col] <= 0).any():\n",
        "            raise ValueError(f\"Column {col} contains non-positive values, cannot apply log transform.\")\n",
        "\n",
        "        # Apply natural logarithm transformation\n",
        "        df[col] = np.log(df[col])\n",
        "\n",
        "        # Initialize and fit MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        df[col] = scaler.fit_transform(df[[col]])\n",
        "\n",
        "        # Save the scaler\n",
        "        scalers[col] = scaler\n",
        "\n",
        "    ic(f\"Data preprocessing completed in {time.time() - start_time_preprocess:.2f} seconds\")\n",
        "    ic(f\"DataFrame shape: {df.shape}\")\n",
        "\n",
        "    return df, scalers\n",
        "\n",
        "# Lightning Wrapper\n",
        "class LightningWrapper(pl.LightningModule):\n",
        "    def __init__(self, model, criterion, optimizer, scheduler, num_epochs: int, scaler_dict: dict, val_data: pd.DataFrame):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.num_epochs = num_epochs\n",
        "        self.scaler_dict = scaler_dict\n",
        "        self.val_data = val_data  # For making predictions during logging\n",
        "        self.validation_rewards = []  # Initialize validation rewards list\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        return self.model(src, tgt)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        batch_X, batch_y = batch\n",
        "        # Shift target sequence to the right and prepend zeros\n",
        "        tgt_input = torch.zeros_like(batch_y)\n",
        "        tgt_input[:, 1:, :] = batch_y[:, :-1, :]\n",
        "        y_pred = self.model(batch_X, tgt_input)\n",
        "        final_loss, mse_loss, mae_loss, perc_diff_loss, max_diff_loss, imbalance_loss, direction_loss, reward, perc_cutoff = self.criterion(y_pred, batch_y)\n",
        "\n",
        "        # Log all loss components\n",
        "        self.log('train/final_loss', final_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "        self.log('train/mse_loss', mse_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('train/mae_loss', mae_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('train/perc_diff_loss', perc_diff_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('train/max_diff_loss', max_diff_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('train/imbalance_loss', imbalance_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('train/direction_loss', direction_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('train/reward', reward, on_step=False, on_epoch=True, sync_dist=True)\n",
        "\n",
        "        return final_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        batch_X, batch_y = batch\n",
        "        # Shift target sequence to the right and prepend zeros\n",
        "        tgt_input = torch.zeros_like(batch_y)\n",
        "        tgt_input[:, 1:, :] = batch_y[:, :-1, :]\n",
        "        y_pred = self.model(batch_X, tgt_input)\n",
        "        final_loss, mse_loss, mae_loss, perc_diff_loss, max_diff_loss, imbalance_loss, direction_loss, reward, percentile_cutoff = self.criterion(y_pred, batch_y)\n",
        "\n",
        "        # Log all loss components\n",
        "        self.log('val_loss', final_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
        "        self.log('val/mse_loss', mse_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/mae_loss', mae_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/perc_diff_loss', perc_diff_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/max_diff_loss', max_diff_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/imbalance_loss', imbalance_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/direction_loss', direction_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/reward', reward, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log('val/perc_cutoff', percentile_cutoff, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.validation_rewards.append(reward.item())\n",
        "\n",
        "        return final_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return {\n",
        "            'optimizer': self.optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': self.scheduler,\n",
        "                'monitor': 'val_loss'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Skip ALL logic during sanity check\n",
        "        if hasattr(self.trainer, 'running_sanity_check') and self.trainer.running_sanity_check:\n",
        "            self.print(\"Skipping ALL on_validation_epoch_end logic during sanity check.\")\n",
        "            return  # Exit the method early\n",
        "\n",
        "        if self.global_rank == 0:\n",
        "            try:\n",
        "                plot_path = self.generate_and_log_plots()\n",
        "                if plot_path:\n",
        "                    img = Image.open(plot_path)\n",
        "                    self.logger.experiment.log({\n",
        "                        \"Validation/Prediction_vs_Target\": wandb.Image(img),\n",
        "                        \"global_step\": self.global_step\n",
        "                    })\n",
        "                    # os.remove(plot_path)\n",
        "            except Exception as e:\n",
        "                self.print(f\"Error in generate_and_log_plots: {e}\")\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # Only the main process should perform logging\n",
        "        if self.global_rank == 0:\n",
        "            # Log learning rate\n",
        "            optimizer = self.optimizers()\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            self.logger.experiment.log({'learning_rate': lr, 'epoch': self.current_epoch})\n",
        "\n",
        "    # def on_after_backward(self):\n",
        "    #     # Only the main process should perform logging\n",
        "    #     if self.global_rank == 0:\n",
        "    #         total_norm = 0.0\n",
        "    #         for p in self.model.parameters():\n",
        "    #             if p.grad is not None:\n",
        "    #                 param_norm = p.grad.detach().data.norm(2)\n",
        "    #                 total_norm += param_norm.item() ** 2\n",
        "    #         total_norm = total_norm ** 0.5\n",
        "    #         self.logger.experiment.log({'Gradients/grad_total_norm': total_norm, 'step': self.global_step})\n",
        "    def on_after_backward(self):\n",
        "        # Only the main process should perform logging\n",
        "        if self.global_rank == 0:\n",
        "            total_norm = 0.0\n",
        "            clip_value = 50.0  # Your gradient clipping value\n",
        "\n",
        "            for p in self.model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.detach().data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "            total_norm = total_norm ** 0.5\n",
        "\n",
        "            # Log total gradient norm\n",
        "            self.logger.experiment.log({'Gradients/grad_total_norm': total_norm, 'step': self.global_step})\n",
        "\n",
        "            # Log whether the gradients were clipped\n",
        "            clipped = total_norm > clip_value\n",
        "            # convert to float to plot in wandb\n",
        "            clipped = float(clipped)\n",
        "            self.logger.experiment.log({'Gradients/clipped': clipped, 'step': self.global_step})\n",
        "\n",
        "    def generate_and_log_plots(self):\n",
        "        \"\"\"\n",
        "        Generate prediction vs target plots and save them to a temporary file.\n",
        "        Returns the path to the saved image.\n",
        "        \"\"\"\n",
        "        # Make predictions on a random sample from validation data\n",
        "        sample = get_random_sample(self.val_data)\n",
        "        input_data, target = sample\n",
        "        input_tensor = prepare_input(input_data, self.device)\n",
        "        tgt_input = torch.zeros_like(target).unsqueeze(0).to(self.device)\n",
        "        prediction = self.model(input_tensor, tgt_input)\n",
        "        _, target_np, prediction_np = convert_to_numpy(input_tensor, target, prediction)\n",
        "\n",
        "        # Prepare DataFrame for plotting\n",
        "        start_idx = random.randint(0, len(self.val_data) - cfg.N_PAST - cfg.N_FUTURE)\n",
        "        past_df = self.val_data.iloc[start_idx:start_idx + cfg.N_PAST]\n",
        "\n",
        "        # Get future data to form the target data\n",
        "        future_df = self.val_data.iloc[start_idx + cfg.N_PAST: start_idx + cfg.N_PAST + cfg.N_FUTURE]\n",
        "\n",
        "        # Ensure past_df and future_df have correct lengths\n",
        "        if len(past_df) < cfg.N_PAST or len(future_df) < cfg.N_FUTURE:\n",
        "            print(\"Not enough data for plotting.\")\n",
        "            return None\n",
        "\n",
        "        # Create filtered_df for plotting: combining past and future data\n",
        "        filtered_df = pd.concat([past_df, future_df])\n",
        "\n",
        "        # Generate and save plot\n",
        "        image_path = visualize_predictions(target_np, prediction_np, cfg.N_FUTURE, self.scaler_dict, filtered_df, cfg.MODEL_SAVE_PATH)\n",
        "\n",
        "        return image_path\n",
        "\n",
        "    def set_perc_cutoff(self, perc_cutoff):\n",
        "        self.criterion.set_perc_cutoff(perc_cutoff)\n",
        "\n",
        "    def get_perc_cutoff(self):\n",
        "        return self.criterion.get_perc_cutoff()\n",
        "\n",
        "# Main Execution Block\n",
        "if __name__ == \"__main__\":\n",
        "    torch.set_float32_matmul_precision(\"medium\")\n",
        "    # Load and preprocess data\n",
        "    df, scalers = load_and_preprocess_data(cfg.DATA_FILE, cfg.DATA_URL)\n",
        "    NUM_FEATURES = df.shape[1]\n",
        "\n",
        "    # Initialize the Wandb logger and name your Wandb project\n",
        "    logger = WandbLogger(project='my-awesome-project', log_model=True)  # Set log_model to True\n",
        "\n",
        "    # Log hyperparameters to Wandb\n",
        "    logger.log_hyperparams({\n",
        "        \"batch_size\": cfg.BATCH_SIZE,\n",
        "        \"hidden_size\": cfg.HIDDEN_SIZE,\n",
        "        \"num_layers\": cfg.NUM_LAYERS,\n",
        "        \"num_epochs\": cfg.NUM_EPOCHS,\n",
        "        \"learning_rate\": 4e-5,\n",
        "        \"weight_decay\": 5e-5\n",
        "    })\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_size = int(0.8 * len(df))\n",
        "    train_data = df.iloc[:train_size]\n",
        "    val_data = df.iloc[train_size:]\n",
        "\n",
        "    # Create Datasets\n",
        "    train_dataset = CryptoDataset(train_data, cfg.N_PAST, cfg.N_FUTURE)\n",
        "    val_dataset = CryptoDataset(val_data, cfg.N_PAST, cfg.N_FUTURE)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize Transformer Model\n",
        "    model = CryptoTransformer(\n",
        "        input_size=NUM_FEATURES,\n",
        "        d_model=cfg.HIDDEN_SIZE,\n",
        "        nhead=8,\n",
        "        num_encoder_layers=cfg.NUM_LAYERS,\n",
        "        num_decoder_layers=cfg.NUM_LAYERS,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.2,\n",
        "        activation=\"gelu\",\n",
        "        n_future=cfg.N_FUTURE,\n",
        "        num_outputs=NUM_FEATURES,\n",
        "        max_seq_length=cfg.N_PAST + cfg.N_FUTURE\n",
        "    ).to(cfg.DEVICE)\n",
        "\n",
        "    # Initialize Loss Function\n",
        "    criterion = BalancedCryptoLoss(cfg)\n",
        "\n",
        "    # Initialize Optimizer and Scheduler\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=4e-5, weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=4, min_lr=4e-6)\n",
        "\n",
        "        # Handle Hot Restart# Create the CosineAnnealingLR scheduler\n",
        "    # Create the CosineAnnealingWarmRestarts scheduler\n",
        "    scheduler = CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=12,           # First restart after 50 epochs\n",
        "        T_mult=2,         # Double the cycle length after each restart\n",
        "        eta_min=1e-6,     # Minimum learning rate\n",
        "        last_epoch=-1,    # Start from the beginning1\n",
        "    )    # ... (rest of your code)\n",
        "\n",
        "    # Handle Hot Restart\n",
        "    if cfg.HOT_RESTART:\n",
        "        try:\n",
        "            # Load the checkpoint into LightningWrapper\n",
        "            wrapped_model = LightningWrapper.load_from_checkpoint(\n",
        "                checkpoint_path=cfg.MODEL_PATH,\n",
        "                model=model,  # Pass your model instance\n",
        "                criterion=criterion,  # Pass your criterion instance\n",
        "                optimizer=optimizer,  # Pass your optimizer instance\n",
        "                scheduler=scheduler,  # Pass your scheduler instance\n",
        "                num_epochs=cfg.NUM_EPOCHS,\n",
        "                scaler_dict=scalers,\n",
        "                val_data=val_data\n",
        "            )\n",
        "\n",
        "            # Access the underlying CryptoTransformer model if needed\n",
        "            model = wrapped_model.model\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"No checkpoint found at {cfg.MODEL_PATH}. Starting fresh.\")\n",
        "            wrapped_model = LightningWrapper(\n",
        "                model=model,\n",
        "                criterion=criterion,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=scheduler,\n",
        "                num_epochs=cfg.NUM_EPOCHS,\n",
        "                scaler_dict=scalers,\n",
        "                val_data=val_data\n",
        "            )\n",
        "    # ... (rest of your code)\n",
        "    if cfg.TRAIN_FIRST:\n",
        "        # Initialize Lightning Wrapper\n",
        "        wrapped_model = LightningWrapper(\n",
        "            model=model,\n",
        "            criterion=criterion,  # Pass the initialized criterion\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            num_epochs=cfg.NUM_EPOCHS,\n",
        "            scaler_dict=scalers,\n",
        "            val_data=val_data\n",
        "        )\n",
        "\n",
        "        # Initialize Callbacks\n",
        "        early_stopping_callback = EarlyStopping(\n",
        "            monitor='val_loss',  # Ensure this matches the logged metric\n",
        "            patience=75,\n",
        "            mode='min'\n",
        "        )\n",
        "\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            monitor='val_loss',  # Ensure this matches the logged metric\n",
        "            dirpath=cfg.MODEL_SAVE_PATH,\n",
        "            filename=f'model-{cfg.VERSION_N}-{{epoch:02d}}-{{val_loss:.2f}}',\n",
        "            save_top_k=9,\n",
        "            mode='min',\n",
        "            save_weights_only=False\n",
        "        )\n",
        "\n",
        "        # Initialize Progress Bar Callback\n",
        "        progress_bar = RichProgressBar(refresh_rate=2)  # Set your desired refresh rate\n",
        "\n",
        "        # Initialize Percentile Cutoff Callback\n",
        "        perc_cutoff_callback = UpdatePercentileCutoffCallback(\n",
        "            reduction_threshold=0.8, # Set your desired reduction threshold of the reward\n",
        "            reduction_factor=0.95\n",
        "        )\n",
        "\n",
        "        # Initialize Trainer with Wandb logger and all callbacks\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=cfg.NUM_EPOCHS,\n",
        "            logger=logger,  # Use Wandb logger here\n",
        "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "            devices=torch.cuda.device_count() if torch.cuda.is_available() else 1,\n",
        "            strategy='ddp_find_unused_parameters_true' if torch.cuda.device_count() > 1 else 'ddp_notebook',  # Distributed Data Parallel\n",
        "            callbacks=[progress_bar, checkpoint_callback, early_stopping_callback, perc_cutoff_callback],\n",
        "            enable_progress_bar=True,\n",
        "            log_every_n_steps=10,\n",
        "            # precision=16,  # Optional: Use mixed precision for faster training\n",
        "            gradient_clip_val=50.0,  # Optional: Gradient clipping\n",
        "        )\n",
        "\n",
        "        # Start Training\n",
        "        trainer.fit(wrapped_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "    else:\n",
        "        print(\"Skipping training as TRAIN_FIRST is set to False.\")\n",
        "\n",
        "    # Finalizing WandB\n",
        "    wandb.finish()\n",
        "\n",
        "    # Clean up CUDA cache\n",
        "    try:\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to empty CUDA cache: {e}\")\n",
        "        pass\n",
        "\n",
        "    # Terminate the script\n",
        "    raise Exception(\"Training completed and script terminated.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16N_-KEYGv6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}